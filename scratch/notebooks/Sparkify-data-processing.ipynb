{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Sparkify\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data file\n",
    "path = \"medium-sparkify-event-data.json\"\n",
    "user_data = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema\n",
    "user_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martin Orford', auth='Logged In', firstName='Joseph', gender='M', itemInSession=20, lastName='Morales', length=597.55057, level='free', location='Corpus Christi, TX', method='PUT', page='NextSong', registration=1532063507000, sessionId=292, song='Grand Designs', status=200, ts=1538352011000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='293')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First row of user data\n",
    "user_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table from data frame. Allows us to use the Spark sql api to interact with the data\n",
    "user_data.createOrReplaceTempView(\"user_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|          location|method|    page| registration|sessionId|         song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+\n",
      "|Martin Orford|Logged In|   Joseph|     M|           20| Morales|597.55057| free|Corpus Christi, TX|   PUT|NextSong|1532063507000|      292|Grand Designs|   200|1538352011000|\"Mozilla/5.0 (Mac...|   293|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a query to retrieve all columns of the first row of user data\n",
    "query = \"\"\"select * from user_data_table\"\"\"\n",
    "spark.sql(query).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------+----------+-----+\n",
      "|userId|gender|registration|      auth| page|\n",
      "+------+------+------------+----------+-----+\n",
      "|      |  null|        null|Logged Out| Home|\n",
      "|      |  null|        null|Logged Out| Home|\n",
      "|      |  null|        null|Logged Out| Home|\n",
      "|      |  null|        null|Logged Out|Login|\n",
      "|      |  null|        null|Logged Out| Home|\n",
      "+------+------+------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Are there any missing users? These are possibly people browsing the website and can be removed\n",
    "# from the data set\n",
    "spark.sql(\"\"\"select userId, gender, registration, auth, page \n",
    "from user_data_table \n",
    "where userId is null or userId = ''\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the missing users and update the table\n",
    "spark.sql(\"\"\"select * \n",
    "from user_data_table \n",
    "where userId is null or userId <> ''\"\"\").createOrReplaceTempView(\"user_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+\n",
      "|userId| registration|           ts|\n",
      "+------+-------------+-------------+\n",
      "|    10|1538159495000|1538965220000|\n",
      "|    10|1538159495000|1538965243000|\n",
      "|    10|1538159495000|1538965485000|\n",
      "|    10|1538159495000|1538965724000|\n",
      "|    10|1538159495000|1538965881000|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When did users register and when did they first start using the site?\n",
    "spark.sql(\"\"\"select userId, registration, ts \n",
    "from user_data_table \n",
    "order by userId, ts\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+\n",
      "|userId| registration|           ts|\n",
      "+------+-------------+-------------+\n",
      "|100051|1542205030000|1542143416000|\n",
      "|100051|1542205030000|1542143492000|\n",
      "|100051|1542205030000|1542143515000|\n",
      "|100051|1542205030000|1542143852000|\n",
      "|100051|1542205030000|1542143853000|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start time cannot be earlier than registration but there are such users in the data file\n",
    "spark.sql(\"\"\"select userId, registration, ts \n",
    "from user_data_table \n",
    "where registration > ts\n",
    "order by userId, ts\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     100|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many such users are there?\n",
    "spark.sql(\"\"\"select count(*) \n",
    "from user_data_table \n",
    "where registration > ts\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove these records and update the data table\n",
    "spark.sql(\"\"\"select * \n",
    "from user_data_table\n",
    "where registration <= ts\"\"\").createOrReplaceTempView(\"user_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "|            Settings|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pages visited by the user\n",
    "spark.sql(\"\"\"\n",
    "select distinct page\n",
    "from user_data_table\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Churn**: We define churn as users visiting the 'Cancellation Confirmation' page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of churned users\n",
    "is_churned = spark.sql(\"\"\"select userId, 'yes' as churned\n",
    "from user_data_table\n",
    "where page = 'Cancellation Confirmation'\n",
    "\"\"\")\n",
    "\n",
    "is_churned.createOrReplaceTempView(\"churned_users_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      98|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of churned users\n",
    "spark.sql(\"\"\"select count(*) from churned_users_table\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge churned users back into the main table and update the table\n",
    "churned = spark.sql(\"\"\"select t1.*, t2.churned\n",
    "from user_data_table t1\n",
    "left join churned_users_table t2\n",
    "on t1.userId = t2.userId\n",
    "order by userId, ts\"\"\")\n",
    "\n",
    "churned = churned.na.fill({'churned': 'no'})\n",
    "churned.createOrReplaceTempView(\"user_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|churned|count(1)|\n",
      "+-------+--------+\n",
      "|     no|     349|\n",
      "|    yes|      98|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of churned users vs non-churned users\n",
    "spark.sql(\"\"\"select churned, count(*) \n",
    "from (\n",
    "        select distinct userId, churned\n",
    "        from user_data_table\n",
    "        where userId <> ''\n",
    ")\n",
    "group by churned\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create averages per user for page visits by day and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create month and date variables from ts\n",
    "spark.udf.register(\"get_month\", lambda x: int(datetime.datetime.fromtimestamp(x / 1000.0).month))\n",
    "spark.udf.register(\"get_date\", lambda x: datetime.datetime.utcfromtimestamp(x / 1000.0).strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select t.userId, \n",
    "avg(sum_NextSong) avg_nextsong_per_day,\n",
    "avg(sum_Submit_Downgrade) avg_submit_downgrade_per_day,\n",
    "avg(sum_Thumbs_Down) avg_sum_thumbs_down_per_day,\n",
    "avg(sum_Home) avg_sum_home_per_day,\n",
    "avg(sum_Downgrade) avg_sum_downgrade_per_day,\n",
    "avg(sum_Roll_Advert) avg_sum_roll_advert_per_day,\n",
    "avg(sum_Logout) avg_sum_logout_per_day,\n",
    "avg(sum_Save_Settings) avg_sum_save_settings_per_day,\n",
    "avg(sum_About) avg_sum_about_per_day,\n",
    "avg(sum_Settings) avg_sum_settings_per_day,\n",
    "avg(sum_Add_to_Playlist) avg_sum_add_playlist_per_day,\n",
    "avg(sum_Friend) avg_sum_add_friend_per_day,\n",
    "avg(sum_Thumbs_Up) avg_sum_thumbs_up_per_day,\n",
    "avg(sum_Help) avg_sum_help_per_day,\n",
    "avg(sum_Upgrade) avg_sum_upgrade_per_day,\n",
    "avg(sum_Error) avg_sum_error_per_day,\n",
    "avg(sum_Submit_Upgrade) avg_submit_upgrade_per_day\n",
    "\n",
    "from \n",
    "(\n",
    "    select userId, \n",
    "    -- get_month(ts) month,\n",
    "    get_date(ts) date,\n",
    "    sum(case when page = 'NextSong' then 1 else 0 end) sum_NextSong,\n",
    "    sum(case when page = 'Submit Downgrade' then 1 else 0 end) sum_Submit_Downgrade,\n",
    "    sum(case when page = 'Thumbs Down' then 1 else 0 end) sum_Thumbs_Down,\n",
    "    sum(case when page = 'Home' then 1 else 0 end) sum_Home,\n",
    "    sum(case when page = 'Downgrade' then 1 else 0 end) sum_Downgrade,\n",
    "    sum(case when page = 'Roll Advert' then 1 else 0 end) sum_Roll_Advert,\n",
    "    sum(case when page = 'Logout' then 1 else 0 end) sum_Logout,\n",
    "    sum(case when page = 'Save Settings' then 1 else 0 end) sum_Save_Settings,\n",
    "    sum(case when page = 'About' then 1 else 0 end) sum_About,\n",
    "    sum(case when page = 'Settings' then 1 else 0 end) sum_Settings,\n",
    "    sum(case when page = 'Add to Playlist' then 1 else 0 end) sum_Add_to_Playlist,\n",
    "    sum(case when page = 'Add Friend' then 1 else 0 end) sum_Friend,\n",
    "    sum(case when page = 'Thumbs Up' then 1 else 0 end) sum_Thumbs_Up,\n",
    "    sum(case when page = 'Help' then 1 else 0 end) sum_Help,\n",
    "    sum(case when page = 'Upgrade' then 1 else 0 end) sum_Upgrade,\n",
    "    sum(case when page = 'Error' then 1 else 0 end) sum_Error,\n",
    "    sum(case when page = 'Submit Upgrade' then 1 else 0 end) sum_Submit_Upgrade\n",
    "    \n",
    "    from user_data_table\n",
    "    group by userId, date\n",
    ") t\n",
    "group by t.userId\n",
    "order by t.userId\n",
    "\"\"\"\n",
    "\n",
    "page_avg_per_day = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select t.userId, \n",
    "avg(sum_NextSong) avg_nextsong_per_month,\n",
    "avg(sum_Submit_Downgrade) avg_submit_downgrade_per_month,\n",
    "avg(sum_Thumbs_Down) avg_sum_thumbs_down_per_month,\n",
    "avg(sum_Home) avg_sum_home_month,\n",
    "avg(sum_Downgrade) avg_sum_downgrade_per_month,\n",
    "avg(sum_Roll_Advert) avg_sum_roll_advert_per_month,\n",
    "avg(sum_Logout) avg_sum_logout_per_month,\n",
    "avg(sum_Save_Settings) avg_sum_save_settings_per_month,\n",
    "avg(sum_About) avg_sum_about_per_month,\n",
    "avg(sum_Settings) avg_sum_settings_per_month,\n",
    "avg(sum_Add_to_Playlist) avg_sum_add_playlist_per_month,\n",
    "avg(sum_Friend) avg_sum_add_friend_per_month,\n",
    "avg(sum_Thumbs_Up) avg_sum_thumbs_up_per_month,\n",
    "avg(sum_Help) avg_sum_help_per_month,\n",
    "avg(sum_Upgrade) avg_sum_upgrade_per_month,\n",
    "avg(sum_Error) avg_sum_error_per_month,\n",
    "avg(sum_Submit_Upgrade) avg_submit_upgrade_per_month\n",
    "\n",
    "from \n",
    "(\n",
    "    select userId, \n",
    "    get_month(ts) month,\n",
    "    -- get_date(ts) date,\n",
    "    sum(case when page = 'NextSong' then 1 else 0 end) sum_NextSong,\n",
    "    sum(case when page = 'Submit Downgrade' then 1 else 0 end) sum_Submit_Downgrade,\n",
    "    sum(case when page = 'Thumbs Down' then 1 else 0 end) sum_Thumbs_Down,\n",
    "    sum(case when page = 'Home' then 1 else 0 end) sum_Home,\n",
    "    sum(case when page = 'Downgrade' then 1 else 0 end) sum_Downgrade,\n",
    "    sum(case when page = 'Roll Advert' then 1 else 0 end) sum_Roll_Advert,\n",
    "    sum(case when page = 'Logout' then 1 else 0 end) sum_Logout,\n",
    "    sum(case when page = 'Save Settings' then 1 else 0 end) sum_Save_Settings,\n",
    "    sum(case when page = 'About' then 1 else 0 end) sum_About,\n",
    "    sum(case when page = 'Settings' then 1 else 0 end) sum_Settings,\n",
    "    sum(case when page = 'Add to Playlist' then 1 else 0 end) sum_Add_to_Playlist,\n",
    "    sum(case when page = 'Add Friend' then 1 else 0 end) sum_Friend,\n",
    "    sum(case when page = 'Thumbs Up' then 1 else 0 end) sum_Thumbs_Up,\n",
    "    sum(case when page = 'Help' then 1 else 0 end) sum_Help,\n",
    "    sum(case when page = 'Upgrade' then 1 else 0 end) sum_Upgrade,\n",
    "    sum(case when page = 'Error' then 1 else 0 end) sum_Error,\n",
    "    sum(case when page = 'Submit Upgrade' then 1 else 0 end) sum_Submit_Upgrade\n",
    "    \n",
    "    from user_data_table\n",
    "    group by userId, month\n",
    ") t\n",
    "group by t.userId\n",
    "order by t.userId\n",
    "\"\"\"\n",
    "\n",
    "page_avg_per_month = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables for averages\n",
    "page_avg_per_day.createOrReplaceTempView(\"page_avg_per_day_table\")\n",
    "page_avg_per_month.createOrReplaceTempView(\"page_avg_per_month_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     F|     197|\n",
      "|     M|     250|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine gender distribution\n",
    "spark.sql(\"\"\"select gender, count(*)\n",
    "from \n",
    "(\n",
    "    select distinct userId, gender from user_data_table)\n",
    "    group by gender\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|churned|gender|count(1)|\n",
      "+-------+------+--------+\n",
      "|     no|     F|     153|\n",
      "|     no|     M|     196|\n",
      "|    yes|     F|      44|\n",
      "|    yes|     M|      54|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gender distribution by churned/non-churned. There does not appear to be much difference in churn rates by gender\n",
    "spark.sql(\"\"\"select churned, gender, count(*)\n",
    "from \n",
    "(select distinct userId, gender, churned from user_data_table)\n",
    "group by churned, gender\n",
    "order by churned, gender\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                page|num_visits|\n",
      "+--------------------+----------+\n",
      "|            NextSong|    432808|\n",
      "|           Thumbs Up|     23824|\n",
      "|                Home|     19084|\n",
      "|     Add to Playlist|     12347|\n",
      "|          Add Friend|      8086|\n",
      "|         Roll Advert|      7760|\n",
      "|              Logout|      5988|\n",
      "|         Thumbs Down|      4909|\n",
      "|           Downgrade|      3811|\n",
      "|            Settings|      2963|\n",
      "|                Help|      2644|\n",
      "|               About|      1025|\n",
      "|             Upgrade|       968|\n",
      "|       Save Settings|       585|\n",
      "|               Error|       503|\n",
      "|      Submit Upgrade|       287|\n",
      "|    Submit Downgrade|       117|\n",
      "|              Cancel|        98|\n",
      "|Cancellation Conf...|        98|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Users visit the NextSong page most frequently\n",
    "spark.sql(\"\"\"select page, count(*) num_visits\n",
    "from user_data_table\n",
    "group by page\n",
    "order by num_visits desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a separate column 'hour' and then calculate the average number of times the user visits the 'NextSong' per hour during his or her time on the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hour variable and update table \n",
    "spark.udf.register(\"get_hour\", lambda x: int(datetime.datetime.fromtimestamp(x / 1000.0).hour))\n",
    "\n",
    "spark.sql('''\n",
    "          SELECT *, get_hour(ts) AS hour\n",
    "          FROM user_data_table \n",
    "          '''\n",
    "          ).createOrReplaceTempView(\"user_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of visits to the NextSong page for all users and update table\n",
    "next_song_visits = spark.sql(\"\"\"select t.userId as userId, \n",
    "avg(t.visits_next_songs) avg_visits_next_songs, \n",
    "max(churned) as churned\n",
    "from\n",
    "(\n",
    "   select userId, page, hour, count(*) as visits_next_songs, max(churned) as churned\n",
    "    from user_data_table \n",
    "    where page = 'NextSong'\n",
    "    group by userId, page, hour\n",
    "    order by cast(hour as int) asc\n",
    ") t\n",
    "group by t.userId\n",
    "order by t.userId\n",
    "\"\"\")\n",
    "\n",
    "next_song_visits.createOrReplaceTempView(\"next_song_visits_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average mins on site per day by user and update table\n",
    "spark.sql(\"\"\"select t2.userId, \n",
    "avg(t2.mins_on_date) avg_mins_on_date\n",
    "\n",
    "from\n",
    "(\n",
    "    select t1.userId, t1.date, sum(t1.mins_in_session) mins_on_date\n",
    "    from\n",
    "        (\n",
    "            select userId, \n",
    "            get_date(ts) date,\n",
    "            sessionId,\n",
    "            ((max(ts) - min(ts))/1000)/60 mins_in_session\n",
    "            from user_data_table\n",
    "            group by 1, 2, 3\n",
    "            order by 1, 2, 3\n",
    "        ) t1\n",
    "    group by 1, 2\n",
    "    order by 1, 2\n",
    ") t2\n",
    "group by 1\n",
    "order by 1\n",
    "\"\"\").createOrReplaceTempView(\"mins_on_date_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|level|count(1)|\n",
      "+-----+--------+\n",
      "| free|  109861|\n",
      "| paid|  418044|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of paid vs free; most observations involve paid users\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "select level, count(*)\n",
    "from user_data_table\n",
    "group by level\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given user what proportion of time spent on the site is spent as a paid user? We calculate the total session minutes per user and calculate the proportion of that time the user spent at the paid level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with number of minutes spent in each session per user.\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select userId, \n",
    "sessionId, \n",
    "((max(ts) - min(ts))/1000)/60 mins_in_session\n",
    "from user_data_table\n",
    "group by userId, sessionId\n",
    "order by userId, sessionId\n",
    "\"\"\").createOrReplaceTempView(\"mins_on_site_per_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much time does the user spend on the site as a paid vs free user?\n",
    "\n",
    "query = \"\"\"\n",
    "select t.userId, t.level, ((sum(t.ts_in_session))/1000)/60 mins_in_session, max(t.churned) churned\n",
    "from\n",
    "(\n",
    "    select userId, sessionId, level, max(ts) - min(ts) ts_in_session, max(churned) churned\n",
    "    from user_data_table\n",
    "    group by userId, sessionId, level\n",
    "    order by userId, sessionId, level\n",
    ") t\n",
    "group by t.userId, t.level\n",
    "order by t.userId, t.level\n",
    "\"\"\"\n",
    "spark.sql(query).createOrReplaceTempView(\"paid_free_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of time a given user spends as a paid user and create table\n",
    "\n",
    "query = \"\"\"\n",
    "select t1.*, t2.total_mins_in_session, \n",
    "(t1.mins_in_session/t2.total_mins_in_session) * 100 as paid_pct,\n",
    "t2.total_mins_in_session/t2.num_sessions avg_mins_per_session\n",
    "\n",
    "from paid_free_table t1\n",
    "join\n",
    "(\n",
    "    select userId, sum(mins_in_session) total_mins_in_session, count(*) num_sessions\n",
    "    from mins_on_site_per_session\n",
    "    group by userId\n",
    "    order by userId\n",
    ") t2\n",
    "on t1.userId = t2.userId and t1.level = 'paid'\n",
    "order by t1.userId\n",
    "\"\"\"\n",
    "spark.sql(query).createOrReplaceTempView(\"paid_pct_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does location matter? We group users by regions.\n",
    "# We split 'location', keep information on the state and map it to regions\n",
    "\n",
    "user_data_df = spark.sql(\"\"\"select * from user_data_table\"\"\")\n",
    "user_data_df2 = user_data_df.withColumn('location', F.split('location', ', ')[1])\n",
    "user_data_df2.createOrReplaceTempView(\"user_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map states to regions and update user table\n",
    "spark.sql(\"\"\"\n",
    "select *,\n",
    "case when location in ('NC-SC', 'TN-MS-AR', 'TN-VA', 'OK', 'FL', 'TX', 'WV', 'AL', 'MD-WV',\n",
    "                        'VA', 'GA', 'VA-NC', 'TN', 'MS', 'KY', 'SC', 'GA-AL', 'LA', 'MD', 'DC-VA-MD-WV') then 'south'\n",
    "     when location in ('RI-MA', 'NJ', 'PA-NJ', 'NY-NJ-PA', 'NH', 'CT', 'PA', 'NY', 'PA-NJ-DE-MD',\n",
    "                         'PA-NJ-DE-MD', 'MA-NH') then 'northeast'\n",
    "     when location in ('UT', 'AR', 'CO', 'NV', 'WA', 'OR-WA', 'OR', 'CA', 'AK'\n",
    "                         'OH', 'IN', 'IL', 'NE-IA', 'UT-ID') then 'west'\n",
    "else 'midwest'\n",
    "end as region\n",
    "from user_data_table\n",
    "\"\"\").createOrReplaceTempView(\"user_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+\n",
      "|churned|   region|count(1)|\n",
      "+-------+---------+--------+\n",
      "|     no|  midwest|      88|\n",
      "|     no|northeast|      57|\n",
      "|     no|    south|     111|\n",
      "|     no|     west|      93|\n",
      "|    yes|  midwest|      23|\n",
      "|    yes|northeast|      22|\n",
      "|    yes|    south|      35|\n",
      "|    yes|     west|      18|\n",
      "+-------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distribution of churned vs non-churned users by region\n",
    "query = \"\"\"select churned, region, count(*)\n",
    "from (\n",
    "    select distinct userId, region, churned\n",
    "    from user_data_table\n",
    ")\n",
    "group by churned, region\n",
    "order by churned, region\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visiting certain pages may be considered as positive engagement with the site\n",
    "# What proportion of visits are positive?\n",
    "\n",
    "query = \"\"\"select userId, \n",
    "sum(num_interactions) total_interactions, \n",
    "sum(pos_engagement) total_pos_engagement,\n",
    "sum(pos_engagement)/sum(num_interactions) * 100 pct_total_pos_engagement,\n",
    "max(churned) churned\n",
    "from (\n",
    "        select userId, sessionId, count(*) num_interactions,\n",
    "        sum(case when page in (\"Add to Playlist\", \"Add Friend\", \n",
    "                            \"NextSong\", \"Thumbs Up\", \"Upgrade\", \"Submit Upgrade\") then 1\n",
    "        else 0\n",
    "        end) as pos_engagement, max(churned) churned\n",
    "        from user_data_table\n",
    "        group by userId, sessionId\n",
    "        order by userId, sessionId\n",
    "    ) t\n",
    "group by userId\n",
    "order by userId\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).createOrReplaceTempView(\"pct_pos_engagement_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an aggregate table where each row is a user and each column is a nominal or aggregate statistic. This table will be used for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with aggregate data\n",
    "query = \"\"\"select distinct userId, gender, region, churned from user_data_table\"\"\"\n",
    "spark.sql(query).createOrReplaceTempView(\"user_data_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the percentage of paid user time table\n",
    "query = \"\"\"select t1.*, t2.paid_pct \n",
    "from user_data_agg_table t1\n",
    "left join (\n",
    "        select userId, paid_pct\n",
    "        from paid_pct_table\n",
    ") t2\n",
    "on t1.userId = t2.userId\n",
    "\"\"\"\n",
    "\n",
    "user_data_agg = spark.sql(query)\n",
    "user_data_agg = user_data_agg.na.fill({'paid_pct': 0.0})\n",
    "user_data_agg.createOrReplaceTempView(\"user_data_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the positive engagements table\n",
    "query = \"\"\"select t1.*, t2.pct_total_pos_engagement \n",
    "from user_data_agg_table t1\n",
    "join (\n",
    "        select userId, pct_total_pos_engagement\n",
    "        from pct_pos_engagement_table\n",
    ") t2\n",
    "on t1.userId = t2.userId\n",
    "\"\"\"\n",
    "\n",
    "user_data_agg = spark.sql(query)\n",
    "user_data_agg.createOrReplaceTempView(\"user_data_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the average number of visits to the NextSong page table\n",
    "query = \"\"\"select t1.*, t2.avg_visits_next_songs \n",
    "from user_data_agg_table t1\n",
    "join (\n",
    "        select userId, avg_visits_next_songs\n",
    "        from next_song_visits_table\n",
    ") t2\n",
    "on t1.userId = t2.userId\n",
    "\"\"\"\n",
    "\n",
    "user_data_agg = spark.sql(query)\n",
    "user_data_agg.createOrReplaceTempView(\"user_data_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the average number of minutes table\n",
    "query = \"\"\"select t1.*, t2.avg_mins_on_date \n",
    "from user_data_agg_table t1\n",
    "join (\n",
    "        select userId, avg_mins_on_date\n",
    "        from mins_on_date_table\n",
    ") t2\n",
    "on t1.userId = t2.userId\n",
    "\"\"\"\n",
    "\n",
    "user_data_agg = spark.sql(query)\n",
    "user_data_agg.createOrReplaceTempView(\"user_data_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the average visits to different pages per day table\n",
    "query = \"\"\"select t1.churned,\n",
    "t1.gender, \n",
    "t1.region,\n",
    "t1.paid_pct,\n",
    "t1.pct_total_pos_engagement,\n",
    "t1.avg_visits_next_songs,\n",
    "t1.avg_mins_on_date,\n",
    "t2.userId,\n",
    "t2.avg_nextsong_per_day,\n",
    "t2.avg_submit_downgrade_per_day,\n",
    "t2.avg_sum_thumbs_down_per_day,\n",
    "t2.avg_sum_home_per_day,\n",
    "t2.avg_sum_downgrade_per_day,\n",
    "t2.avg_sum_roll_advert_per_day,\n",
    "t2.avg_sum_logout_per_day,\n",
    "t2.avg_sum_save_settings_per_day,\n",
    "t2.avg_sum_about_per_day,\n",
    "t2.avg_sum_settings_per_day,\n",
    "t2.avg_sum_add_playlist_per_day,\n",
    "t2.avg_sum_add_friend_per_day,\n",
    "t2.avg_sum_thumbs_up_per_day,\n",
    "t2.avg_sum_help_per_day,\n",
    "t2.avg_sum_upgrade_per_day,\n",
    "t2.avg_sum_error_per_day,\n",
    "t2.avg_submit_upgrade_per_day\n",
    "\n",
    "from user_data_agg_table t1\n",
    "join (\n",
    "        select *\n",
    "        from page_avg_per_day_table\n",
    ") t2\n",
    "on t1.userId = t2.userId\n",
    "\"\"\"\n",
    "\n",
    "user_data_agg = spark.sql(query)\n",
    "user_data_agg.createOrReplaceTempView(\"user_data_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the average visits to different pages per month table\n",
    "query = \"\"\"select t1.*,\n",
    "t2.avg_nextsong_per_month,\n",
    "t2.avg_submit_downgrade_per_month,\n",
    "t2.avg_sum_thumbs_down_per_month,\n",
    "t2.avg_sum_home_month,\n",
    "t2.avg_sum_downgrade_per_month,\n",
    "t2.avg_sum_roll_advert_per_month,\n",
    "t2.avg_sum_logout_per_month,\n",
    "t2.avg_sum_save_settings_per_month,\n",
    "t2.avg_sum_about_per_month,\n",
    "t2.avg_sum_settings_per_month,\n",
    "t2.avg_sum_add_playlist_per_month,\n",
    "t2.avg_sum_add_friend_per_month,\n",
    "t2.avg_sum_thumbs_up_per_month,\n",
    "t2.avg_sum_help_per_month,\n",
    "t2.avg_sum_upgrade_per_month,\n",
    "t2.avg_sum_error_per_month,\n",
    "t2.avg_submit_upgrade_per_month\n",
    "\n",
    "from user_data_agg_table t1\n",
    "join (\n",
    "        select *\n",
    "        from page_avg_per_month_table\n",
    ") t2\n",
    "on t1.userId = t2.userId\n",
    "\"\"\"\n",
    "\n",
    "user_data_agg = spark.sql(query)\n",
    "user_data_agg.createOrReplaceTempView(\"user_data_agg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe with aggregate data\n",
    "path = \"user_data_agg.json\"\n",
    "user_data_agg.write.json(path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
